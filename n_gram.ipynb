{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9decdb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd63d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('en_wiki_small.txt', encoding='utf-8')\n",
    "raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86959a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token = sent_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1f15bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_set, test_set = train_test_split(corpus, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d96377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a3affa3",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d0e177a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213175"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_corpus = []\n",
    "for sent in sent_token[:]:\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'[^\\w\\s]', '', sent)\n",
    "    sent = re.sub(r'[\\n\\[\\]\\(\\)]', '', sent)\n",
    "    sent = re.sub(r'(\\d+\\.\\d+|\\d+g|\\d+)', '', sent)\n",
    "    sent_corpus.append(sent)\n",
    "\n",
    "len(sent_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e435d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     sent = word_tokenize(sent)\n",
    "#     sent = [t for t in sent if t not in stopwords.words('english')]\n",
    "#     sent = set(sent)\n",
    "#     sent = list(sent)\n",
    "#     [sent_corpus.append(sentence) for sentence in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e94a61b",
   "metadata": {},
   "source": [
    "## TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca36e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(sent_corpus, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd471b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191857"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3d67686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21318"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8bcfa",
   "metadata": {},
   "source": [
    "## FIVE SETS OF THE TRAIN-DEV SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37db5f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1, dev_1 = train_test_split(train_set[:38371], test_size=0.1, random_state=42)\n",
    "train_2, dev_2 = train_test_split(train_set[38371:76742], test_size=0.1, random_state=42)\n",
    "train_3, dev_3 = train_test_split(train_set[76742:115113], test_size=0.1, random_state=42)\n",
    "train_4, dev_4 = train_test_split(train_set[115113:153484], test_size=0.1, random_state=42)\n",
    "train_5, dev_5 = train_test_split(train_set[153484:], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b534249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34533"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56f19bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34533"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "51431565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################################\n",
      "\n",
      "\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 57 items>\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "\n",
    "paddedLines = []\n",
    "\n",
    "for sent in train_1[:5]:\n",
    "    sent = word_tokenize(sent)\n",
    "    sent = [t for t in sent if t not in stopwords.words('english')]\n",
    "    sent = set(sent)\n",
    "    sent = list(sent)\n",
    "    sent = list(pad_both_ends(sent, n=3))\n",
    "    sent = list(ngrams(sent, 3))\n",
    "    [paddedLines.append(item) for item in sent]\n",
    "    \n",
    "for items in paddedLines:\n",
    "#     print(items)\n",
    "    for unk in items:\n",
    "        if(unk == '<<UNK>>'):\n",
    "            print(unk)\n",
    "        \n",
    "\n",
    "# print(unk_found)\n",
    "# print(paddedLines)\n",
    "\n",
    "vocab = nltk.lm.Vocabulary(paddedLines)\n",
    "print(\"###########################################################\")\n",
    "print()\n",
    "print()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e01541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c185a110",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Ngram <<UNK>> isn't a tuple, but <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[135], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLE\n\u001b[0;32m      2\u001b[0m trump_model \u001b[38;5;241m=\u001b[39m MLE(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# Lets train a 3-grams model, previously we set n=3\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrump_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaddedLines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\Understanding_NLTK\\nltk_venv\\lib\\site-packages\\nltk\\lm\\api.py:116\u001b[0m, in \u001b[0;36mLanguageModel.fit\u001b[1;34m(self, text, vocabulary_text)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    113\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot fit without a vocabulary or text to create it from.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    114\u001b[0m         )\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mupdate(vocabulary_text)\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcounts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\Understanding_NLTK\\nltk_venv\\lib\\site-packages\\nltk\\lm\\counter.py:119\u001b[0m, in \u001b[0;36mNgramCounter.update\u001b[1;34m(self, ngram_text)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m sent:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ngram, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    120\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNgram <\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m> isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt a tuple, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ngram, \u001b[38;5;28mtype\u001b[39m(ngram))\n\u001b[0;32m    121\u001b[0m         )\n\u001b[0;32m    123\u001b[0m     ngram_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ngram)\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngram_order \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Ngram <<UNK>> isn't a tuple, but <class 'str'>"
     ]
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "trump_model = MLE(3) # Lets train a 3-grams model, previously we set n=3\n",
    "trump_model.fit(paddedLines, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "144badaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = trump_model.generate(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8c1fbd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'housing', '</s>', '</s>', '</s>']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee95d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nltk_venv",
   "language": "python",
   "name": "nltk_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
